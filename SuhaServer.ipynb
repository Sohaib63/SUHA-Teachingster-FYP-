{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VSHhxT26fHMt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79b90b1f-6499-4c38-be99-3a400330e695"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-sIF6cR3gQMw"
      },
      "outputs": [],
      "source": [
        "mkdir pytorch-flask-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SBHkq7e-gZFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c53046c5-6bc2-4ea8-fd08-55176fde743b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-flask-api\n"
          ]
        }
      ],
      "source": [
        "cd pytorch-flask-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7S6YdYklfo19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cbb0d3c-1dae-404f-89c3-983389428527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/pytorch-flask-api.zip\n",
            "   creating: .git/\n",
            "   creating: .ipynb_checkpoints/\n",
            "   creating: __pycache__/\n",
            "   creating: genderage_v1/\n",
            "   creating: models/\n",
            "   creating: mtcnn-model/\n",
            "   creating: persons/\n",
            "  inflating: model-symbol.json       \n",
            "  inflating: preprocess.py           \n",
            "  inflating: model-0000.params       \n",
            "  inflating: model-0000.zip          \n",
            "  inflating: genderage_v1.zip        \n",
            "  inflating: .gitignore              \n",
            "  inflating: imagenet_class_index.json  \n",
            "  inflating: helper.py               \n",
            "  inflating: README.md               \n",
            "  inflating: requirements.txt        \n",
            "  inflating: models.zip              \n",
            "  inflating: app.py                  \n",
            "  inflating: your_file_name.jpg      \n",
            "  inflating: mtcnn-model.zip         \n",
            "  inflating: LICENSE                 \n",
            "  inflating: face_preprocess.py      \n",
            "  inflating: mtcnn_detector.py       \n",
            "  inflating: face_model.py           \n",
            "  inflating: __pycache__/face_model.cpython-37.pyc  \n",
            "  inflating: __pycache__/mtcnn_detector.cpython-37.pyc  \n",
            "  inflating: __pycache__/face_preprocess.cpython-37.pyc  \n",
            "  inflating: __pycache__/helper.cpython-37.pyc  \n",
            "  inflating: genderage_v1/model-symbol.json  \n",
            "  inflating: genderage_v1/model-0000.params  \n",
            "  inflating: models/weights.149-0.01.hdf5  \n",
            "  inflating: models/models.zip       \n",
            "  inflating: models/68_face_landmarks_predictor.dat  \n",
            "  inflating: mtcnn-model/det2.prototxt  \n",
            "  inflating: mtcnn-model/det1-0001.params  \n",
            "  inflating: mtcnn-model/det3.prototxt  \n",
            "  inflating: mtcnn-model/det1-symbol.json  \n",
            "  inflating: mtcnn-model/det1.prototxt  \n",
            "  inflating: mtcnn-model/det4.caffemodel  \n",
            "  inflating: mtcnn-model/det2.caffemodel  \n",
            "  inflating: mtcnn-model/det3-symbol.json  \n",
            "  inflating: mtcnn-model/det4-symbol.json  \n",
            "  inflating: mtcnn-model/det1.caffemodel  \n",
            "  inflating: mtcnn-model/det2-0001.params  \n",
            "  inflating: mtcnn-model/det3.caffemodel  \n",
            "  inflating: mtcnn-model/det4-0001.params  \n",
            "  inflating: mtcnn-model/det3-0001.params  \n",
            "  inflating: mtcnn-model/det2-symbol.json  \n",
            "  inflating: mtcnn-model/det4.prototxt  \n",
            "   creating: .git/branches/\n",
            "   creating: .git/hooks/\n",
            "   creating: .git/info/\n",
            "   creating: .git/logs/\n",
            "   creating: .git/objects/\n",
            "   creating: .git/refs/\n",
            "  inflating: .git/description        \n",
            "  inflating: .git/packed-refs        \n",
            "  inflating: .git/config             \n",
            "  inflating: .git/index              \n",
            "  inflating: .git/HEAD               \n",
            "  inflating: .git/hooks/pre-push.sample  \n",
            "  inflating: .git/hooks/post-update.sample  \n",
            "  inflating: .git/hooks/pre-rebase.sample  \n",
            "  inflating: .git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: .git/hooks/pre-commit.sample  \n",
            "  inflating: .git/hooks/applypatch-msg.sample  \n",
            "  inflating: .git/hooks/commit-msg.sample  \n",
            "  inflating: .git/hooks/pre-applypatch.sample  \n",
            "  inflating: .git/hooks/update.sample  \n",
            "  inflating: .git/hooks/pre-receive.sample  \n",
            "  inflating: .git/hooks/fsmonitor-watchman.sample  \n",
            "   creating: .git/logs/refs/\n",
            "  inflating: .git/logs/HEAD          \n",
            "   creating: .git/logs/refs/heads/\n",
            "   creating: .git/logs/refs/remotes/\n",
            "   creating: .git/logs/refs/remotes/origin/\n",
            "  inflating: .git/logs/refs/remotes/origin/HEAD  \n",
            "  inflating: .git/logs/refs/heads/master  \n",
            "  inflating: .git/info/exclude       \n",
            "   creating: .git/refs/heads/\n",
            "   creating: .git/refs/remotes/\n",
            "   creating: .git/refs/tags/\n",
            "   creating: .git/refs/remotes/origin/\n",
            "  inflating: .git/refs/remotes/origin/HEAD  \n",
            "  inflating: .git/refs/heads/master  \n",
            "   creating: .git/objects/14/\n",
            "   creating: .git/objects/1f/\n",
            "   creating: .git/objects/25/\n",
            "   creating: .git/objects/32/\n",
            "   creating: .git/objects/3b/\n",
            "   creating: .git/objects/3d/\n",
            "   creating: .git/objects/46/\n",
            "   creating: .git/objects/54/\n",
            "   creating: .git/objects/5f/\n",
            "   creating: .git/objects/62/\n",
            "   creating: .git/objects/6a/\n",
            "   creating: .git/objects/6c/\n",
            "   creating: .git/objects/9d/\n",
            "   creating: .git/objects/a0/\n",
            "   creating: .git/objects/b2/\n",
            "   creating: .git/objects/be/\n",
            "   creating: .git/objects/cd/\n",
            "   creating: .git/objects/ea/\n",
            "   creating: .git/objects/f3/\n",
            "   creating: .git/objects/info/\n",
            "   creating: .git/objects/pack/\n",
            "  inflating: .git/objects/1f/a961fae6e982d85b333ff6e60ac78ac03a36ec  \n",
            "  inflating: .git/objects/46/f61964c91eab69746a98c8b5922a630726e8dd  \n",
            "  inflating: .git/objects/54/ad7b399360fdfde71f09bb4611dbf56b21f91b  \n",
            "  inflating: .git/objects/ea/40261b2726c5f0562541ad6b897a771649a7f4  \n",
            "  inflating: .git/objects/be/6e167478a2a8bd722c2facc1c6a2d8b2f8a3f4  \n",
            "  inflating: .git/objects/6a/d8b7ed9c4042e2d0f11e36f8bdf7a376850999  \n",
            "  inflating: .git/objects/5f/e0dfefcd3dca3b1d169c7ab51b93de327e07e2  \n",
            "  inflating: .git/objects/6c/4ce2afccb4e3005e8f6c3ded3db17febd2ee2b  \n",
            "  inflating: .git/objects/cd/5fd67d46f759dbe3b51b903c4cbecabdb616b8  \n",
            "  inflating: .git/objects/a0/e68a1f3e12c81b4db202be2a0ac271959b2e4d  \n",
            "  inflating: .git/objects/32/34830b86ad1c36045e970dfdc5db253fe036d5  \n",
            "  inflating: .git/objects/14/0ab6d75e568a8baeb2ca296ea59a4d81dd2b44  \n",
            "  inflating: .git/objects/25/120175780df5f7311a8ff5bb14e8de30216fd3  \n",
            "  inflating: .git/objects/3b/9b2f91d60c021436abc04c77222399cce143bf  \n",
            "  inflating: .git/objects/b2/d76182304e3cf4d714f42a82e1f2c4382ab11c  \n",
            "  inflating: .git/objects/9d/328892f4af8a2ead3629cb9736591e7c427275  \n",
            "  inflating: .git/objects/62/99a37294d40a7dedffb873f3c321f21bc561ca  \n",
            "  inflating: .git/objects/f3/dd15fa1efb2ae03fe4071ab0e791f77b01b803  \n",
            "  inflating: .git/objects/3d/17c2218f1a30ff6568f0428f1cf2fd5909bbe2  \n",
            "  inflating: persons/ahmed.jpeg      \n",
            "  inflating: persons/sohaib.jpeg     \n",
            "  inflating: persons/habib.jpg       \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/drive/MyDrive/pytorch-flask-api.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qlV-hNQ93jLj"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/rajeevratan84/pytorch-flask-api.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRcn0zC739z5"
      },
      "source": [
        "## **Install ngrok**\n",
        "\n",
        "ngrok is cross-platform application that exposes local server ports to the Internet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OiKRQ_hJ3jXg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2034e4-5c66-4a31-be12-7a673c68f34b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.2.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from flask-ngrok) (2.27.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.3.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-ngrok) (8.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->flask-ngrok) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.8->flask-ngrok) (2.1.2)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nw5PsSUg4C78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cfadd8e-cb61-4e5b-ed81-1e91dc2536d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-18 00:35:00--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 54.161.241.46, 52.202.168.65, 18.205.222.128, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|54.161.241.46|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13856790 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.tgz’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.21M  13.9MB/s    in 1.0s    \n",
            "\n",
            "2023-05-18 00:35:01 (13.9 MB/s) - ‘ngrok-stable-linux-amd64.tgz’ saved [13856790/13856790]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xTVt6HsQ37Eo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c12382fd-283c-411a-94ba-42032427b78e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:2 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n",
            "Hit:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:7 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Get:8 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Get:12 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,693 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,342 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,169 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,046 kB]\n",
            "Get:17 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [2,252 B]\n",
            "Fetched 8,612 kB in 3s (3,442 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "24 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  ngrok\n",
            "0 upgraded, 1 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 6,420 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.3.0 [6,420 kB]\n",
            "Fetched 6,420 kB in 2s (3,127 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ngrok.\n",
            "(Reading database ... 122531 files and directories currently installed.)\n",
            "Preparing to unpack .../archives/ngrok_3.3.0_amd64.deb ...\n",
            "Unpacking ngrok (3.3.0) ...\n",
            "Setting up ngrok (3.3.0) ...\n"
          ]
        }
      ],
      "source": [
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeFHGl345KcV"
      },
      "source": [
        "Sign up link - https://dashboard.ngrok.com/signup?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ptqts0XP2-cF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9617757d-306c-4665-94c1-b66718e7bd6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 2EPHxjj0A4GzQWPjCUz2TQEAnHB_3pevb8PRCcfKDugmnRLaN\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rRFDp9ougR94"
      },
      "outputs": [],
      "source": [
        "# %cd pytorch-flask-api/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "to7A84_3bjCC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e8fb359-90af-4447-a954-2a05fbf0c9b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.1.3)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.10/dist-packages (from face_recognition) (19.24.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from face_recognition) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from face_recognition) (8.4.0)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566173 sha256=4ee23a646f43c91f03188f2287ea97a253bd0147366a1f783f3ee5e7cfba67d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/eb/cf/e9eced74122b679557f597bb7c8e4c739cfcac526db1fd523d\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R-dScc47cj-Q"
      },
      "outputs": [],
      "source": [
        "import face_recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "W6D6UyPQf9_q"
      },
      "outputs": [],
      "source": [
        "# !unzip genderage_v1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "XnK3re3HGEo-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d779005-7560-4104-9686-f0a37693cece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.79-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.22.4)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from deepface) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.65.0)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.6.6)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (8.4.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from deepface) (4.7.0.72)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.12.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.12.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from deepface) (2.2.4)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.13-py3-none-any.whl (16 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire>=0.4.0->deepface) (2.3.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.3.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=1.1.2->deepface) (8.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (3.12.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=3.10.1->deepface) (4.11.2)\n",
            "Requirement already satisfied: setuptools>=3.0 in /usr/local/lib/python3.10/dist-packages (from gunicorn>=20.1.0->deepface) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->deepface) (2022.7.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.4.8)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (3.20.3)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (2.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=1.9.0->deepface) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.9.0->deepface) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=1.9.0->deepface) (1.10.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=1.1.2->deepface) (2.1.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (1.8.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.4.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=1.9.0->deepface) (3.2.2)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=5908cf89abf33b710353e7896ce380bac197355d5f6ca9656cdc0e6eee574883\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "Successfully built fire\n",
            "Installing collected packages: gunicorn, fire, mtcnn, retina-face, deepface\n",
            "Successfully installed deepface-0.0.79 fire-0.5.0 gunicorn-20.1.0 mtcnn-0.1.1 retina-face-0.0.13\n"
          ]
        }
      ],
      "source": [
        "!pip install deepface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EAV-C-pQjkvt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f232cecc-e185-466f-9ca9-b29bedf2b78e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mxnet\n",
            "  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.22.4)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.27.1)\n",
            "Collecting graphviz<0.9.0,>=0.8.1 (from mxnet)\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.4)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.20.1\n",
            "    Uninstalling graphviz-0.20.1:\n",
            "      Successfully uninstalled graphviz-0.20.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mxnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "devA6x1l57ap"
      },
      "outputs": [],
      "source": [
        "# !unzip mtcnn-model.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bRjC24GbbVDH"
      },
      "outputs": [],
      "source": [
        "# !unzip model-0000.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HZvBf1XQnpAH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a53ee9-d6af-490e-bc30-460ee335c108"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['model', '0000']\n",
            "loading model 0\n",
            "['genderage_v1/model', '0000']\n",
            "loading genderage_v1/model 0\n"
          ]
        }
      ],
      "source": [
        "import face_model\n",
        "import argparse\n",
        "import cv2\n",
        "from time import sleep\n",
        "import os\n",
        "import unicodedata\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def eu_dist(x, y):\n",
        "    d = 0.0\n",
        "    for i in range(len(x)):\n",
        "        d += pow((float(x[i]) - float(y[i])), 2)\n",
        "    d = math.sqrt(d)\n",
        "    return d/len(x)\n",
        "\n",
        "\n",
        "def save_encodings(imgs, name, model):\n",
        "    global known_encodings, known_names\n",
        "    for i in range(0, len(imgs)):\n",
        "        im = cv2.imread(imgs[i])\n",
        "        img, bbox = model.get_input(im, th=0.98)\n",
        "        if img is not None:\n",
        "            f2 = model.get_feature(img[0])\n",
        "            known_encodings.append(f2)\n",
        "            known_names.append(name[i])\n",
        "        else:\n",
        "            print('Warning: No face Detected for '+name[i]+'\\n')\n",
        "\n",
        "\n",
        "def str_to_ndarray(input):\n",
        "    temp = input.split(',')\n",
        "    temp = temp[0:len(temp)-1]\n",
        "    for t in range(0, len(temp)):\n",
        "        temp[t] = float(unicodedata.normalize(\n",
        "            'NFKD', temp[t]).encode('ascii', 'ignore'))\n",
        "    temp = np.asarray(temp)\n",
        "    return temp\n",
        "\n",
        "\n",
        "known_encodings = []\n",
        "known_names = []\n",
        "\n",
        "\n",
        "def get_encodings():\n",
        "    global known_names\n",
        "    global known_encodings\n",
        "    known_encodings = []\n",
        "    known_names = []\n",
        "    qry = 'select name,encodings from encodings;'\n",
        "    cnx = open_connection()\n",
        "    if cnx:\n",
        "        print('Getting encoding from Database')\n",
        "        cursor = cnx.cursor()\n",
        "        cursor.execute(qry)\n",
        "        data = cursor.fetchall()\n",
        "        for dat in data:\n",
        "            name = dat[0]\n",
        "            encodings = dat[1]\n",
        "            known_encodings.append(str_to_ndarray(encodings))\n",
        "            known_names.append(unicodedata.normalize(\n",
        "                'NFKD', name).encode('ascii', 'ignore'))\n",
        "        cursor.reset()\n",
        "        close_connection(cnx)\n",
        "        known_encodings = np.asarray(known_encodings)\n",
        "        try:\n",
        "            os.remove(\"encodings.npy\")\n",
        "            os.remove(\"names.npy\")\n",
        "            np.save('encodings.npy', known_encodings)\n",
        "            np.save('names.npy', known_names)\n",
        "            print('Latest Encodings are saved')\n",
        "        except:\n",
        "            print('Unable to store new encodings')\n",
        "        return known_names, known_encodings\n",
        "    else:\n",
        "        print('Getting encodings from file')\n",
        "        get_encodings_from_file()\n",
        "\n",
        "\n",
        "def get_encodings_from_file():\n",
        "    global known_names\n",
        "    global known_encodings\n",
        "    if os.path.exists('./encodings.npy') and os.path.exists('./names.npy'):\n",
        "        known_encodings = np.load('encodings.npy')\n",
        "        known_names = np.load('names.npy')\n",
        "    return known_names, known_encodings\n",
        "\n",
        "\n",
        "def face_recog(test_vec, th):\n",
        "    dis = eu_dist(known_encodings[0], test_vec)\n",
        "    min_dis = dis\n",
        "    name = known_names[0]\n",
        "    for vec in range(1, len(known_encodings)):\n",
        "        dis = eu_dist(known_encodings[vec], test_vec)\n",
        "        # print(known_names[vec])\n",
        "        # print(dis)\n",
        "        if dis < min_dis:\n",
        "            min_dis = dis\n",
        "            name = known_names[vec]\n",
        "    if min_dis < th:\n",
        "        return name\n",
        "    else:\n",
        "        return 'unknown'\n",
        "\n",
        "\n",
        "def FaceMain(image):\n",
        "    result = dict()\n",
        "    global args, model\n",
        "\n",
        "    # print('Done.....')\n",
        "\n",
        "# cap=cv2.VideoCapture('rtsp://admin:admin@10.11.18.180:554/cam/realmonitor?channel=1&subtype=1')\n",
        "    # cap=cv2.VideoCapture(0)\n",
        "    # ret,image=cap.read()\n",
        "\n",
        "# get_encodings_from_file()\n",
        "   # while ret:\n",
        "    # ret,image=cap.read()\n",
        "    img, bbox = model.get_input(image, th=0.98)\n",
        "    if img is not None:\n",
        "        for im in range(0, len(img)):\n",
        "            f2 = model.get_feature(img[im])\n",
        "            fhab = model.get_ga(img[im])\n",
        "            name = face_recog(f2, 0.0021)\n",
        "           # image = cv2.rectangle(image, (int(bbox[im][0]), int(bbox[im][1])), (int(bbox[im][2]), int(bbox[im][3])), (255, 0, 0), 2)\n",
        "            gender, age = fhab\n",
        "            result = {\"name\": name, \"gender\": gender, \"age\": age}\n",
        "            return result\n",
        "            # image = cv2.putText(image, name, (int(bbox[im][0]),int(bbox[im][1])), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='face model test')\n",
        "# # general\n",
        "parser.add_argument('--image-size', default='112,112', help='')\n",
        "parser.add_argument('--model', default='model-0000',\n",
        "                    help='path to load model.')\n",
        "parser.add_argument(\n",
        "    '--ga-model', default='genderage_v1/model-0000', help='path to load model.')\n",
        "parser.add_argument('--gpu', default=0, type=int, help='gpu id')\n",
        "parser.add_argument('--det', default=0, type=int,\n",
        "                    help='mtcnn option, 1 means using R+O, 0 means detect from begining')\n",
        "parser.add_argument('--flip', default=0, type=int,\n",
        "                    help='whether do lr flip aug')\n",
        "parser.add_argument('--threshold', default=1.24,\n",
        "                    type=float, help='ver dist threshold')\n",
        "parser.add_argument(\"-f\", \"--file\", required=False)\n",
        "args = parser.parse_args()\n",
        "model = face_model.FaceModel(args)\n",
        "base_path = r'persons'\n",
        "images_path = []\n",
        "names_list = []\n",
        "for im in os.listdir(base_path):\n",
        "    images_path.append(os.path.join(base_path, im))\n",
        "    names_list.append(im.split('.')[0])\n",
        "save_encodings(images_path, names_list, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_Cu9bhLIb6Lf"
      },
      "outputs": [],
      "source": [
        "# !unzip models.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jLkYIIL5YvA1"
      },
      "outputs": [],
      "source": [
        "from glob import glob\n",
        "from keras import backend as K\n",
        "import imutils\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import keras\n",
        "import requests\n",
        "from scipy.spatial import distance as dist\n",
        "from imutils import face_utils\n",
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "import os\n",
        "import sys\n",
        "import collections\n",
        "import random\n",
        "import face_recognition\n",
        "import pickle\n",
        "import math\n",
        "import threading\n",
        "import tensorflow as tf\n",
        "\n",
        "num_cores = 4\n",
        "num_CPU = 1\n",
        "num_GPU = 1\n",
        "\n",
        "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=num_cores,\n",
        "                                  inter_op_parallelism_threads=num_cores,\n",
        "                                  allow_soft_placement=True,\n",
        "                                  device_count={'CPU': num_CPU,\n",
        "                                                'GPU': num_GPU}\n",
        "                                  )\n",
        "\n",
        "session = tf.compat.v1.Session(config=config)\n",
        "K.set_session(session)\n",
        "\n",
        "\n",
        "class FacialLandMarksPosition:\n",
        "    \"\"\"\n",
        "    The indices points to the various facial features like left ear, right ear, nose, etc.,\n",
        "    that are mapped from the Facial Landmarks used by dlib's FacialLandmarks predictor.\n",
        "    \"\"\"\n",
        "    left_eye_start_index, left_eye_end_index = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
        "    right_eye_start_index, right_eye_end_index = face_utils.FACIAL_LANDMARKS_IDXS[\n",
        "        \"right_eye\"]\n",
        "\n",
        "\n",
        "facial_landmarks_predictor = 'models/68_face_landmarks_predictor.dat'\n",
        "predictor = dlib.shape_predictor(facial_landmarks_predictor)\n",
        "\n",
        "modelForEye = load_model('models/weights.149-0.01.hdf5')\n",
        "\n",
        "\n",
        "def predict_eye_state(model, image):\n",
        "    image = cv2.resize(image, (20, 10))\n",
        "    image = image.astype(dtype=np.float32)\n",
        "\n",
        "    image_batch = np.reshape(image, (1, 10, 20, 1))\n",
        "    image_batch = tf.keras.applications.mobilenet.preprocess_input(image_batch)\n",
        "\n",
        "    return np.argmax(modelForEye.predict(image_batch)[0])\n",
        "\n",
        "#cap = cv2.VideoCapture(0)\n",
        "\n",
        "\n",
        "def eyeMain():\n",
        "    left = None\n",
        "    right = None\n",
        "    scale = 0.5\n",
        "    while(True):\n",
        "        #c = time.time()\n",
        "\n",
        "        # Capture frame-by-frame\n",
        "        #ret, frame = cap.read()\n",
        "        frame = cv2.imread('your_file_name.jpg')\n",
        "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        original_height, original_width = image.shape[:2]\n",
        "\n",
        "        resized_image = cv2.resize(image,  (0, 0), fx=scale, fy=scale)\n",
        "        lab = cv2.cvtColor(resized_image, cv2.COLOR_BGR2LAB)\n",
        "\n",
        "        l, _, _ = cv2.split(lab)\n",
        "\n",
        "        resized_height, resized_width = l.shape[:2]\n",
        "        height_ratio, width_ratio = original_height / \\\n",
        "            resized_height, original_width / resized_width\n",
        "\n",
        "        face_locations = face_recognition.face_locations(l, model='hog')\n",
        "\n",
        "        if len(face_locations):\n",
        "            top, right, bottom, left = face_locations[0]\n",
        "            x1, y1, x2, y2 = left, top, right, bottom\n",
        "\n",
        "            x1 = int(x1 * width_ratio)\n",
        "            y1 = int(y1 * height_ratio)\n",
        "            x2 = int(x2 * width_ratio)\n",
        "            y2 = int(y2 * height_ratio)\n",
        "\n",
        "        # draw face rectangle\n",
        "\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            shape = predictor(gray, dlib.rectangle(x1, y1, x2, y2))\n",
        "\n",
        "            face_landmarks = face_utils.shape_to_np(shape)\n",
        "\n",
        "            left_eye_indices = face_landmarks[FacialLandMarksPosition.left_eye_start_index:\n",
        "                                              FacialLandMarksPosition.left_eye_end_index]\n",
        "\n",
        "            (x, y, w, h) = cv2.boundingRect(np.array([left_eye_indices]))\n",
        "            left_eye = gray[y:y + h, x:x + w]\n",
        "\n",
        "            right_eye_indices = face_landmarks[FacialLandMarksPosition.right_eye_start_index:\n",
        "                                               FacialLandMarksPosition.right_eye_end_index]\n",
        "\n",
        "            (x, y, w, h) = cv2.boundingRect(np.array([right_eye_indices]))\n",
        "            right_eye = gray[y:y + h, x:x + w]\n",
        "\n",
        "            left_eye_open = 'yes' if predict_eye_state(\n",
        "                model=modelForEye, image=left_eye) else 'no'\n",
        "            right_eye_open = 'yes' if predict_eye_state(\n",
        "                model=modelForEye, image=right_eye) else 'no'\n",
        "\n",
        "            left = '{}'.format(left_eye_open)\n",
        "            right = '{}'.format(right_eye_open)\n",
        "        # print(right)\n",
        "        #print('left eye open: {0}    right eye open: {1}'.format(left_eye_open, right_eye_open))\n",
        "            break\n",
        "            if left_eye_open == 'yes' and right_eye_open == 'yes':\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            else:\n",
        "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
        "\n",
        "        #cv2.imshow('right_eye', right_eye)\n",
        "       # cv2.imshow('left_eye', left_eye)\n",
        "\n",
        "        break\n",
        "\n",
        "# When everything done, release the capture\n",
        "# cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    # print(left)\n",
        "    # print(right)\n",
        "    if(left == None or right == None):\n",
        "        return None\n",
        "    return{'left': left, 'right': right}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "j_iPubVac3Ku"
      },
      "outputs": [],
      "source": [
        "# image_file = cv2.imread(r'your_file_name.jpg')\n",
        "# result = FaceMain(image_file)\n",
        "# print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "f_4O0PD3cj72"
      },
      "outputs": [],
      "source": [
        "# eyeMain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6VjLEYASZXMT"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "# using now() to get current time\n",
        "\n",
        "\n",
        "def Datemain():\n",
        "    current_time = datetime.datetime.now()\n",
        "\n",
        "    # Printing attributes of now().\n",
        "    MonthDict = {1: \"January\",\n",
        "                 2: \"February\",\n",
        "                 3: \"March\",\n",
        "                 4: \"April\",\n",
        "                 5: \"May\",\n",
        "                 6: \"June\",\n",
        "                 7: \"July\",\n",
        "                 8: \"August\",\n",
        "                 9: \"September\",\n",
        "                 10: \"October\",\n",
        "                 11: \"November\",\n",
        "                 12: \"December\"\n",
        "                 }\n",
        "\n",
        "    # print(\"Year :\", current_time.year)\n",
        "\n",
        "    # print(\"Month : \", MonthDict[current_time.month])\n",
        "\n",
        "    # print(\"Day : \", current_time.strftime(\"%A\"))\n",
        "\n",
        "    result = \"Today is \" + str(current_time.strftime(\"%A\")) + \" \" + \\\n",
        "        str(MonthDict[current_time.month]) + \" \"+str(current_time.year)\n",
        "    return result\n",
        "# time = datetime.datetime.now().strftime(\"%I:%M %p\")\n",
        "# list = time.split(':')\n",
        "# hours = list[0]\n",
        "# secondPart = list[1]\n",
        "# print(hours)\n",
        "# list = secondPart.split(\" \")\n",
        "# minutes = list[0]\n",
        "# print(minutes)\n",
        "# timeAmOrPm = list[1]\n",
        "# print(timeAmOrPm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Z613__4LZjUE"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import inflect\n",
        "p = inflect.engine()\n",
        "# using now() to get current tim\n",
        "\n",
        "\n",
        "def timeMain():\n",
        "    time = datetime.datetime.now().strftime(\"%I:%M %p\")\n",
        "    list = time.split(':')\n",
        "    hours = list[0]\n",
        "    secondPart = list[1]\n",
        "    # print(hours)\n",
        "    list = secondPart.split(\" \")\n",
        "    minutes = list[0]\n",
        "    print(type(minutes), minutes)\n",
        "\n",
        "    timeAmOrPm = list[1]\n",
        "    if(minutes == 0):\n",
        "        result = \"Its \" + p.number_to_words(int(hours)) + \" \" + \\\n",
        "            str(timeAmOrPm)+\".\"\n",
        "    else:\n",
        "        result = \"Its \" + p.number_to_words(int(hours))+\" \" + \\\n",
        "            p.number_to_words(int(minutes)) + \" \"+str(timeAmOrPm)+\".\"\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Bi-_mWg-ZthZ"
      },
      "outputs": [],
      "source": [
        "# Python program to find current\n",
        "# weather details of any city\n",
        "# using openweathermap api\n",
        "\n",
        "# import required modules\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "\n",
        "\n",
        "def kelvinToCelsius(kelvin):\n",
        "    return kelvin - 273.15\n",
        "\n",
        "\n",
        "# Enter your API key here\n",
        "api_key = \"ef2117b97a49f2cea243693ddc683db3\"\n",
        "\n",
        "# base_url variable to store url\n",
        "base_url = \"http://api.openweathermap.org/data/2.5/weather?\"\n",
        "\n",
        "# Give city name\n",
        "city_name = \"Lahore\"\n",
        "\n",
        "# complete_url variable to store\n",
        "# complete url address\n",
        "complete_url = base_url + \"appid=\" + api_key + \"&q=\" + city_name\n",
        "\n",
        "# get method of requests module\n",
        "# return response object\n",
        "\n",
        "\n",
        "def weatherMain():\n",
        "    response = requests.get(complete_url)\n",
        "\n",
        "    # json method of response object\n",
        "    # convert json format data into\n",
        "    # python format data\n",
        "    x = response.json()\n",
        "\n",
        "    # Now x contains list of nested dictionaries\n",
        "    # Check the value of \"cod\" key is equal to\n",
        "    # \"404\", means city is found otherwise,\n",
        "    # city is not found\n",
        "    if x[\"cod\"] != \"404\":\n",
        "\n",
        "        # store the value of \"main\"\n",
        "        # key in variable y\n",
        "        y = x[\"main\"]\n",
        "\n",
        "        # store the value corresponding\n",
        "        # to the \"temp\" key of y\n",
        "        current_temperature = y[\"temp\"]\n",
        "        feels_like = y['feels_like']\n",
        "\n",
        "        # store the value corresponding\n",
        "        # to the \"pressure\" key of y\n",
        "        current_pressure = y[\"pressure\"]\n",
        "\n",
        "        # store the value corresponding\n",
        "        # to the \"humidity\" key of y\n",
        "        current_humidity = y[\"humidity\"]\n",
        "\n",
        "        # store the value of \"weather\"\n",
        "        # key in variable z\n",
        "        z = x[\"weather\"]\n",
        "\n",
        "        # store the value corresponding\n",
        "        # to the \"description\" key at\n",
        "        # the 0th index of z\n",
        "        weather_description = z[0][\"description\"]\n",
        "\n",
        "        # print following values\n",
        "        result = str(weather_description) + \" is now currently in Lahore. Lahore's current temperature of \" + str(math.floor(\n",
        "            kelvinToCelsius(current_temperature))) + \" degrees Celsius feels like \" + str(math.floor(\n",
        "                kelvinToCelsius(feels_like))) + \" celsius. The atmospheric pressure is \" + str(current_pressure) + \" hydraulic pascal. The air is \" + str(current_humidity) + \" percent humid.\"\n",
        "        return result\n",
        "    # print(\" Temperature (in kelvin unit) = \" +\n",
        "    #                     str(current_temperature) +\n",
        "    #           \"\\n atmospheric pressure (in hPa unit) = \" +\n",
        "    #                     str(current_pressure) +\n",
        "    #           \"\\n humidity (in percentage) = \" +\n",
        "    #                     str(current_humidity) +\n",
        "    #           \"\\n description = \" +\n",
        "    #                     str(weather_description))\n",
        "\n",
        "    else:\n",
        "        #print(\" City Not Found \")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UjXNd_hFdG2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a416c7-9a3f-4201-e643-3df11b763851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory  /root /.deepface created\n",
            "Directory  /root /.deepface/weights created\n",
            "facial_expression_model_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
            "To: /root/.deepface/weights/facial_expression_model_weights.h5\n",
            "100%|██████████| 5.98M/5.98M [00:00<00:00, 11.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "race_model_single_batch.h5 will be downloaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/race_model_single_batch.h5\n",
            "To: /root/.deepface/weights/race_model_single_batch.h5\n",
            "100%|██████████| 537M/537M [00:09<00:00, 58.9MB/s]\n",
            "Action: emotion: 100%|██████████| 2/2 [00:08<00:00,  4.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral\n",
            "middle eastern\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from deepface import DeepFace\n",
        "obj = DeepFace.analyze(img_path=\"your_file_name.jpg\",\n",
        "                       actions=[ 'race', 'emotion'])\n",
        "print(obj[0]['dominant_emotion'])\n",
        "print(obj[0]['dominant_race'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PAEgdkPptosv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6ab4a8d4-569d-41b3-8ab3-b0a54fcfa1d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/pytorch-flask-api'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kXDs0ZJItdRx"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def putlabels(x):\n",
        "\n",
        "    if str(x) == '0':\n",
        "        return 'neutral'\n",
        "    elif str(x) == '1':\n",
        "        return 'happy'\n",
        "    elif str(x) == '2':\n",
        "        return 'sad'\n",
        "    elif str(x) == '3':\n",
        "        return 'surprise'\n",
        "    else:\n",
        "        return 'anger'\n",
        "\n",
        "\n",
        "def net_face(frame, shape):\n",
        "\n",
        "    net = cv2.dnn.readNet(face_model + '.xml', face_model + '.bin')\n",
        "    blob = cv2.dnn.blobFromImage(frame, size=shape, ddepth=cv2.CV_8U)\n",
        "    net.setInput(blob)\n",
        "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
        "\n",
        "    out = net.forward()\n",
        "    return out\n",
        "\n",
        "\n",
        "def net_emote(frame, shape):\n",
        "\n",
        "    net = cv2.dnn.readNet(emote_model + '.xml', emote_model + '.bin')\n",
        "    blob = cv2.dnn.blobFromImage(frame, size=shape, ddepth=cv2.CV_8U)\n",
        "    net.setInput(blob)\n",
        "    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n",
        "\n",
        "    out = net.forward()\n",
        "    return out\n",
        "\n",
        "\n",
        "face_model = 'face-detection-retail-0005'\n",
        "emote_model = 'emotions-recognition-retail-0003'\n",
        "path = 'in.mp4'\n",
        "\n",
        "#cap = cv2.VideoCapture(0)\n",
        "\n",
        "\n",
        "def mainForEmotion():\n",
        "    frame = cv2.imread(\"your_file_name.jpg\")\n",
        "#frame_width = int(cap.get(3))\n",
        "#frame_height = int(cap.get(4))\n",
        "\n",
        "\n",
        "#writer = cv2.VideoWriter('out.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 25, (frame_width,frame_height))\n",
        "    returnvalue = None\n",
        "    while(True):\n",
        "        #ret, frame = cap.read()\n",
        "\n",
        "        # if ret == False:\n",
        "        #print('End of video')\n",
        "        # break\n",
        "\n",
        "        out = net_face(frame, (300, 300))\n",
        "\n",
        "        for detection in out.reshape(-1, 7):\n",
        "\n",
        "            conf = float(detection[2])\n",
        "            xmin = int(detection[3] * frame.shape[1])\n",
        "            ymin = int(detection[4] * frame.shape[0])\n",
        "            xmax = int(detection[5] * frame.shape[1])\n",
        "            ymax = int(detection[6] * frame.shape[0])\n",
        "\n",
        "            if conf > 0.65:\n",
        "\n",
        "                soft = net_emote(frame, (64, 64))\n",
        "                emo_pred = np.reshape(soft, (5,))\n",
        "                emo_pred = list(emo_pred)\n",
        "                returnvalue = putlabels(emo_pred.index(max(emo_pred)))\n",
        "            #cv2.rectangle(frame, (xmin,ymin),(xmax,ymax), (0,128,255), 2)\n",
        "            #cv2.putText(frame, putlabels(emo_pred.index(max(emo_pred))) , (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "\n",
        "        # cv2.imshow('Emotion-Recognition',frame)\n",
        "        break\n",
        "    # writer.write(frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    return returnvalue\n",
        "    # print(returnvalue)\n",
        "\n",
        "# cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# frame=cv2.imread(\"your_file_name.jpg\")\n",
        "# #frame_width = int(cap.get(3))\n",
        "# #frame_height = int(cap.get(4))\n",
        "\n",
        "# #writer = cv2.VideoWriter('out.avi',cv2.VideoWriter_fourcc('M','J','P','G'), 25, (frame_width,frame_height))\n",
        "# returnvalue=None\n",
        "# while(True):\n",
        "#     #ret, frame = cap.read()\n",
        "\n",
        "#     #if ret == False:\n",
        "#         #print('End of video')\n",
        "#        # break\n",
        "\n",
        "#     out = net_face(frame, (300,300))\n",
        "\n",
        "#     for detection in out.reshape(-1,7):\n",
        "\n",
        "#         conf = float(detection[2])\n",
        "#         xmin = int(detection[3] * frame.shape[1])\n",
        "#         ymin = int(detection[4] * frame.shape[0])\n",
        "#         xmax = int(detection[5] * frame.shape[1])\n",
        "#         ymax = int(detection[6] * frame.shape[0])\n",
        "\n",
        "#         if conf > 0.65:\n",
        "\n",
        "#             soft = net_emote(frame,(64,64))\n",
        "#             emo_pred = np.reshape(soft, (5,))\n",
        "#             emo_pred = list(emo_pred)\n",
        "#             returnvalue=putlabels(emo_pred.index(max(emo_pred)))\n",
        "#             #cv2.rectangle(frame, (xmin,ymin),(xmax,ymax), (0,128,255), 2)\n",
        "#             #cv2.putText(frame, putlabels(emo_pred.index(max(emo_pred))) , (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
        "\n",
        "#         #cv2.imshow('Emotion-Recognition',frame)\n",
        "#     break\n",
        "#     #writer.write(frame)\n",
        "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "#         break\n",
        "# print(returnvalue)\n",
        "# #cap.release()\n",
        "# cv2.destroyAllWindows()\n",
        "# #writer.release()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "uuECLWWXxbGh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "180a8aa9-7af8-48aa-c2f0-0d5d50a03cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencv-python-inference-engine\n",
            "  Downloading opencv_python_inference_engine-2022.1.5-py3-none-manylinux1_x86_64.whl (41.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from opencv-python-inference-engine) (1.22.4)\n",
            "Installing collected packages: opencv-python-inference-engine\n",
            "Successfully installed opencv-python-inference-engine-2022.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cv2"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip3 install opencv-python-inference-engine"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !lsof -i :5000"
      ],
      "metadata": {
        "id": "znZBc_1iRVlH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kill 178 "
      ],
      "metadata": {
        "id": "Zpw3-hrMRYZC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-BHguj938lk",
        "outputId": "9cb02ae6-ecd5-4818-e8ac-f15bdb89768e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import json\n",
        "from torchvision import models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from threading import Thread\n",
        "from flask import Flask, jsonify, request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "app = Flask(__name__)\n",
        "# run_with_ngrok(app)\n",
        "\n",
        "\n",
        "from deepface import DeepFace\n",
        "\n",
        "\n",
        "def getRace():\n",
        "    try:\n",
        "      obj = DeepFace.analyze(img_path=\"your_file_name.jpg\",\n",
        "                            actions=['race'])\n",
        "      return obj[0][\"dominant_race\"]\n",
        "    except:\n",
        "      return None;\n",
        "def getEmotion():\n",
        "    try:\n",
        "      obj = DeepFace.analyze(img_path=\"your_file_name.jpg\",\n",
        "                              actions=['emotion'])\n",
        "      return obj[0][\"dominant_emotion\"]\n",
        "    except:\n",
        "      return None;\n",
        "\n",
        "# def transform_image(image_bytes):\n",
        "#     my_transforms = transforms.Compose([transforms.Resize(255),\n",
        "#                                         transforms.CenterCrop(224),\n",
        "#                                         transforms.ToTensor(),\n",
        "#                                         transforms.Normalize(\n",
        "#                                             [0.485, 0.456, 0.406],\n",
        "#                                             [0.229, 0.224, 0.225])])\n",
        "#     image = Image.open(io.BytesIO(image_bytes))\n",
        "#     return my_transforms(image).unsqueeze(0)\n",
        "\n",
        "\n",
        "# def get_prediction(image_bytes):\n",
        "#     tensor = transform_image(image_bytes=image_bytes)\n",
        "#     outputs = model.forward(tensor)\n",
        "#     _, y_hat = outputs.max(1)\n",
        "#     predicted_idx = str(y_hat.item())\n",
        "#     return imagenet_class_index[predicted_idx]\n",
        "\n",
        "\n",
        "@app.route('/race', methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        file = request.files['file']\n",
        "        file.save(\"your_file_name.jpg\")\n",
        "\n",
        "        # img_bytes = file.read()\n",
        "        result=getRace()\n",
        "        # print(type(result))\n",
        "        # print(eyeMain())\n",
        "        # img_bytes = file.read()\n",
        "        # print(FaceMain(img_bytes))\n",
        "        return jsonify({'Race': result})\n",
        "\n",
        "@app.route('/emotion', methods=['POST'])\n",
        "def predictEmotion():\n",
        "    if request.method == 'POST':\n",
        "        file = request.files['file']\n",
        "        file.save(\"your_file_name.jpg\")\n",
        "\n",
        "        # img_bytes = file.read()\n",
        "        # result=m.ismain(image_file)\n",
        "        result=getEmotion()\n",
        "        # print(type(result))\n",
        "        # # print(eyeMain())\n",
        "        # # img_bytes = file.read()\n",
        "        # # print(FaceMain(img_bytes))\n",
        "        return jsonify({'emotion': result})\n",
        "\n",
        "@app.route('/faceGenderAge', methods=['POST'])\n",
        "def predictFaceAgeGender():\n",
        "    if request.method == 'POST':\n",
        "        file = request.files['file']\n",
        "        # file.save(\"your_file_name.jpg\")\n",
        "\n",
        "        # img_bytes = file.read()\n",
        "        # result=getEmotion()\n",
        "        # print(type(result))\n",
        "        # print(eyeMain())\n",
        "        file.save(\"your_file_name.jpg\")\n",
        "        image_file = cv2.imread(r'your_file_name.jpg')\n",
        "        # img_bytes = file.read()\n",
        "        result=FaceMain(image_file)\n",
        "        if(not result):\n",
        "            return {'result':result}\n",
        "        return jsonify({'result':{'name': str(result['name']),\n",
        "                        'age': str(result['age']),\n",
        "                        'gender': str(result['gender'])}})\n",
        "@app.route('/eye', methods=['POST'])\n",
        "def predictEye():\n",
        "    if request.method == 'POST':\n",
        "        file = request.files['file']\n",
        "        # file.save(\"your_file_name.jpg\")\n",
        "\n",
        "        # img_bytes = file.read()\n",
        "        # result=getEmotion()\n",
        "        # print(type(result))\n",
        "        result=eyeMain()\n",
        "        # img_bytes = file.read()\n",
        "        # result=FaceMain(img_bytes)\n",
        "        if(not result):\n",
        "            return {'result':result}\n",
        "        return {'result':result}      \n",
        "@app.route('/date', methods=['POST'])\n",
        "def getDate():\n",
        "  if request.method == 'POST':\n",
        "    result=Datemain()  \n",
        "    return jsonify({'date': result})\n",
        "\n",
        "@app.route('/time', methods=['POST'])\n",
        "def getTime():\n",
        "  if request.method == 'POST':\n",
        "    result=  timeMain()\n",
        "    return jsonify({'time': result})\n",
        "\n",
        "@app.route('/weather', methods=['POST'])\n",
        "def getWeather():\n",
        "  if request.method == 'POST':\n",
        "    result=  weatherMain()\n",
        "    return jsonify(result)\n",
        "\n",
        "@app.route('/testing', methods=['POST'])\n",
        "def getQA():\n",
        "  if request.method == 'POST':\n",
        "    question = request.form['name']\n",
        "\n",
        "    response = openai.Completion.create(\n",
        "            model=\"text-davinci-002\",\n",
        "            prompt=\"You:{}\".format(question),\n",
        "            temperature=0.3,\n",
        "            max_tokens=60,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0.5,\n",
        "            presence_penalty=0,\n",
        "            stop=[\"You:\"]\n",
        "        )\n",
        "    return jsonify({'answer': response})\n",
        "# /testing\n",
        "def run_flask_app():\n",
        "    app.run(port=5000)  # Replace with your desired port number\n",
        "\n",
        "# Start the Flask application in a separate thread\n",
        "flask_thread = Thread(target=run_flask_app)\n",
        "flask_thread.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkFUsXD8n8hH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dad901b-ab4d-4124-f057-69d4055c36d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m[05-18|00:37:02] no configuration paths supplied \n",
            "\u001b[32mINFO\u001b[0m[05-18|00:37:02] using configuration at default config path \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml\n",
            "\u001b[32mINFO\u001b[0m[05-18|00:37:02] open config file                         \u001b[32mpath\u001b[0m=/root/.config/ngrok/ngrok.yml \u001b[32merr\u001b[0m=nil\n",
            "t=2023-05-18T00:37:02+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "t=2023-05-18T00:37:03+0000 lvl=info msg=\"client session established\" obj=tunnels.session obj=csess id=7501e93d9063\n",
            "t=2023-05-18T00:37:03+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2023-05-18T00:37:03+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:5000 url=https://2b3f-34-90-231-45.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [18/May/2023 00:37:26] \"POST /weather HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t=2023-05-18T00:37:26+0000 lvl=info msg=\"join connections\" obj=join id=129b1265d871 l=127.0.0.1:5000 r=72.255.36.69:31988\n",
            "t=2023-05-18T00:37:34+0000 lvl=info msg=\"join connections\" obj=join id=ea11e270d41d l=127.0.0.1:5000 r=72.255.36.69:31988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 10.99it/s]\n",
            "INFO:werkzeug:127.0.0.1 - - [18/May/2023 00:37:35] \"POST /emotion HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t=2023-05-18T00:38:04+0000 lvl=info msg=\"join connections\" obj=join id=03e4cb97c471 l=127.0.0.1:5000 r=72.255.36.69:31988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [18/May/2023 00:38:05] \"POST /faceGenderAge HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t=2023-05-18T00:38:40+0000 lvl=info msg=\"join connections\" obj=join id=e2302fa1a21f l=127.0.0.1:5000 r=72.255.36.69:31988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [18/May/2023 00:38:42] \"POST /faceGenderAge HTTP/1.1\" 200 -\n"
          ]
        }
      ],
      "source": [
        "!ngrok http 5000 --log=stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbw18uLZ6NKP"
      },
      "source": [
        "## **Open new Colab Window and run the following blocks**\n",
        "\n",
        "1. Get our test image.\n",
        "```\n",
        "!wget https://raw.githubusercontent.com/rajeevratan84/ModernComputerVision/main/bird.jpg\n",
        "```\n",
        "\n",
        "2. Send our API Post Requests. Replace the URL below with the NGROK URL above.\n",
        "```\n",
        "import requests\n",
        "resp = requests.post(\"http://xxxxxxxxxxxx.ngrok.io/predict\",\n",
        "                     files={\"file\": open('bird.jpg','rb')})\n",
        "print(resp.json())\n",
        "```\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}